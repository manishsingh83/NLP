{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e367f1d",
   "metadata": {},
   "source": [
    "\n",
    "# NLTK Word_Sentence_Tokenization_Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a498d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678347c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b20f4c4",
   "metadata": {},
   "source": [
    "Tokenization= is a process of breaking down a given paragraph of that into a list of sentences , it is called when\n",
    " paragraph is broken doen into list of sentences, it is called sentences Tokenization , Similarly if the sentences are further \n",
    "    broken down into list of words known as word-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e811e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'studing', 'at', 'DataTrained', 'Instite', 'Noida', '.', 'The', 'name', 'of', 'course', 'is', 'Data', 'science', 'with', 'ML', ',', 'Deep-Learing', 'and', 'business', 'analytics', 'tools']\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "## Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text= \"\"\"I am studing at DataTrained Instite Noida. The name of course is Data science with ML,Deep-Learing and business analytics tools\"\"\"\n",
    "print(word_tokenize(text))\n",
    "print(len(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1d0986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am studing at DataTrained Instite Noida.', 'The name of course is Data science with ML,Deep-Learing and business analytics tools']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "## Sentences Tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "print(len(sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c2107d3",
   "metadata": {},
   "source": [
    "\n",
    "## REGULAR EXPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f44f4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'studing', 'at', 'ata', 'rained', 'nstite', 'can', 't', 'don', 't', 'haven', 't']\n",
      "['am', 'studing', 'at', 'ata', 'rained', 'nstite', '.', 'can', 't', 'don', 't', 'haven', 't']\n"
     ]
    }
   ],
   "source": [
    "text1= \"I am studing at DataTrained Instite NOIDA. @2022  11*  can't don't  haven't\"\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(text1, \"[a-z]+\"))  \n",
    "print(regexp_tokenize(text1, \"[a-z.]+\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05e7ebe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'studing', 'at', 'ata', 'rained', 'nstite', \"can't\", \"don't\", \"haven't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(regexp_tokenize(text1, \"[a-z']+\"))  # [a-z'] its cover can't , don't wouldn't  like word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8337b412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'D', 'T', 'I', 'N', 'O', 'I', 'D', 'A']\n",
      "['I', 'D', 'T', 'I', 'NOIDA']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(regexp_tokenize(text1, \"[A-Z]\"))  # it is seperate the words like  eg. 'NOIDA'\n",
    "print(regexp_tokenize(text1, \"[A-Z]+\"))  ## its cover capital letter word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61f4f438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I am studing at DataTrained Instite NOIDA. @2022  11*  can't don't  haven't\"]\n"
     ]
    }
   ],
   "source": [
    "text1= \"I am studing at DataTrained Instite NOIDA. @2022  11*  can't don't  haven't\"\n",
    "\n",
    "print(regexp_tokenize(text1, \"[\\a-z]+\")) # it give same result that is in  text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43e03e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I ', ' ', ' ', ' D', 'T', ' I', ' NOIDA. @2022  11*  ', \"'\", ' ', \"'\", '  ', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(regexp_tokenize(text1, \"[^a-z]+\"))  #  caret ^ ,it eliminate that character which come before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad32b029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ', '. @2022  11*  ', \"'\", ' ', \"'\", '  ', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(regexp_tokenize(text1, \"[^a-zA-Z]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ef5de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'studing', 'at', 'DataTrained', 'Instite', 'NOIDA', 'can', 't', 'don', 't', 'haven', 't']\n",
      "['I', 'am', 'studing', 'at', 'DataTrained', 'Instite', 'NOIDA', \"can't\", \"don't\", \"haven't\"]\n",
      "['I', 'am', 'studing', 'at', 'DataTrained', 'Instite', 'NOIDA', '2022', '11', \"can't\", \"don't\", \"haven't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(regexp_tokenize(text1, \"[a-zA-Z]+\"))\n",
    "print(regexp_tokenize(text1, \"[a-zA-Z']+\"))  #  \"[a-zA-Z']+\"  ,  \"[a-z'A-Z]+\"  these  are same\n",
    "print(regexp_tokenize(text1, \"[a-z'A-Z0-9]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df0c14f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'studing', 'at', 'DataTrained', 'Instite', 'NOIDA', '@2022', '11', \"can't\", \"don't\", \"haven't\"]\n",
      "['I', 'am', 'studing', 'at', 'DataTrained', 'Instite', 'NOIDA', '@2022', '11', \"can't\", \"don't\", \"haven't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(regexp_tokenize(text1, \"[a-z'A-Z@0-9]+\"))  #  same 1\n",
    "print(regexp_tokenize(text1, \"[a-zA-Z0-9'@]+\"))  #  same 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba3e6cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022', '11']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(regexp_tokenize(text1, \"[0-9]+\"))  # its print only number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d923d",
   "metadata": {},
   "source": [
    "\n",
    "NLP-2  NLTK STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60dabf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first  import stopwords\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f5d4e17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] \n",
      "\n",
      "\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "print(sw, '\\n\\n')\n",
    "print(len(sw))         # total lenght of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a0c35359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##  ADDING CUSTOM STOPWORDS\n",
    "\n",
    "s_w = set(stopwords.words('english'))   # without using set() will not add another words in stopwords \n",
    "s_w.update(('hi','non'))\n",
    "\n",
    "print(len(s_w))  # there are two added in stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d325ebac",
   "metadata": {},
   "source": [
    "\n",
    "# similar to the stopwords , we can ignore punctuation in our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d72a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7143a888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53fccf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "punc= string.punctuation\n",
    "print(len(punc))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10c5d84f",
   "metadata": {},
   "source": [
    "# Remove stopwords & punctuation from the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "67657a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  string\n",
    "stopword= stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "\n",
    "text2= \"\"\"Tokenization= is a @process^ of breaking down a given * paragraphof that into a list! of sentences , it is called when\n",
    " paragraph is broken doen into list of sentences, it is called sentences Tokenization. , Similarly if the sentences are further \n",
    "    broken down% into &list of words $ known as word-tokenization\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac50c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word= []\n",
    "for word in word_tokenize(text2):\n",
    "    if word not in punct:\n",
    "        if word not in stopword:\n",
    "            clean_word.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4007e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization=', 'process^', 'breaking', 'given', 'paragraphof', 'list', 'sentences', 'called', 'paragraph', 'broken', 'doen', 'list', 'sentences', 'called', 'sentences', 'Tokenization', 'Similarly', 'sentences', 'broken', 'list', 'words', 'known', 'word-tokenization']\n"
     ]
    }
   ],
   "source": [
    "print(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "756efc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text3 = \"\"\"Tokenization is= a @process^ of breaking down a given * paragraphof that into a list! of sentences , it is called when\n",
    " paragraph is broken doen into list of sentences, it is called sentences Tokenization. , Similarly if the sentences are further \n",
    "    broken down% into &list of words $ known as word-tokenization\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6889a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_word1= []\n",
    "for word in word_tokenize(text3):\n",
    "    if word not in punct:\n",
    "        if word not in stopword:\n",
    "            clean_word1.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "19ff8b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is=', 'process^', 'breaking', 'given', 'paragraphof', 'list', 'sentences', 'called', 'paragraph', 'broken', 'doen', 'list', 'sentences', 'called', 'sentences', 'Tokenization', 'Similarly', 'sentences', 'broken', 'list', 'words', 'known', 'word-tokenization']\n"
     ]
    }
   ],
   "source": [
    "print(clean_word1)   ##      = ^ these are not seperated  even from word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f09543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c06224",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Cases \n",
    "  means convert into lower case/upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f82f5249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am studing at datatrained instite noida. the name of course is data science with ml,deep-learing and business analytics tools\n",
      "\n",
      "I AM STUDING AT DATATRAINED INSTITE NOIDA. THE NAME OF COURSE IS DATA SCIENCE WITH ML,DEEP-LEARING AND BUSINESS ANALYTICS TOOLS\n"
     ]
    }
   ],
   "source": [
    "print(text.lower())\n",
    "print()\n",
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63e8ec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "NLP-3   NLTK  LEMMATIZATION , STEMMING &  WORDNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ab352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few types of stemmer like  PorterStemmer , LancaterStemmer &  SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "52db5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer,SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "96d76eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter= PorterStemmer()\n",
    "lancaster= LancasterStemmer()\n",
    "snowball= SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2289a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "ran\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "                                        ## porter stemmer\n",
    "print(porter.stem('hobby'))\n",
    "print(porter.stem('hobbies'))\n",
    "print(porter.stem('computer'))\n",
    "print(porter.stem('computation'))\n",
    "print(porter.stem('ran'))\n",
    "print(porter.stem('running'))\n",
    "print(porter.stem('run'))                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5c7f1243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n",
      "comput\n",
      "ran\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lancaster.stem('hobby'))\n",
    "print(lancaster.stem('hobbies'))\n",
    "print(lancaster.stem('computer'))\n",
    "print(lancaster.stem('computation'))\n",
    "print(lancaster.stem('compute'))\n",
    "print(lancaster.stem('ran'))\n",
    "print(lancaster.stem('running'))\n",
    "print(lancaster.stem('run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "98febbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "comput\n",
      "ran\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "##                                    Snowball   Stemmer\n",
    "print(snowball.stem('hobby'))\n",
    "print(snowball.stem('hobbies'))\n",
    "print(snowball.stem('computer'))\n",
    "print(snowball.stem('computation'))\n",
    "print(snowball.stem('compute'))\n",
    "print(snowball.stem('ran'))\n",
    "print(snowball.stem('running'))\n",
    "print(snowball.stem('run'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "74785451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am stude at datatrain instit noida . the name of cours is data scienc with ml , deep-lear and busi analyt tool\n",
      "i am stud at datatrain instit noid . the nam of cours is dat sci with ml , deep-learing and busy analys tool\n",
      "i am stude at datatrain instit noida . the name of cours is data scienc with ml , deep-lear and busi analyt tool\n"
     ]
    }
   ],
   "source": [
    "## comparing bweteen these stemmers\n",
    "token= list(word_tokenize(text))\n",
    "for stemer in (porter,lancaster,snowball):\n",
    "    stem = [stemer.stem(i) for i in token]\n",
    "    print(' '.join(stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cee907",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f958125",
   "metadata": {},
   "source": [
    "= Lemmatization also does the same thing as  STEMMING & try to bring a word to  its base form, but unlike stemming ,\n",
    " it to keep in a   the actual meaning of the base word\n",
    "    \n",
    "# lemmatization take more time to process compare to 'stemming' technique\n",
    "# lemmatize is very complex & takes more time to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3e905eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f354ce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('running'))\n",
    "print(lemma.lemmatize('run'))\n",
    "print(lemma.lemmatize('runs'))\n",
    "print(lemma.lemmatize('ran'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e981d86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(lemma.lemmatize('running', pos='v'))\n",
    "print(lemma.lemmatize('run', pos='v'))\n",
    "print(lemma.lemmatize('runs', pos='v'))\n",
    "print(lemma.lemmatize('ran', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "73ae366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'stude', 'at', 'datatrain', 'instit', 'noida', '.', 'the', 'name', 'of', 'cours', 'is', 'data', 'scienc', 'with', 'ml', ',', 'deep-lear', 'and', 'busi', 'analyt', 'tool']\n"
     ]
    }
   ],
   "source": [
    "##                    PORTER STEMMER\n",
    "poter = [porter.stem(lem) for lem in token]\n",
    "print(poter)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "04dbc7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'studing', 'at', 'DataTrained', 'Instite', 'Noida', '.', 'The', 'name', 'of', 'course', 'is', 'Data', 'science', 'with', 'ML', ',', 'Deep-Learing', 'and', 'business', 'analytics', 'tool']\n"
     ]
    }
   ],
   "source": [
    "#  Lemmatization\n",
    "lematize= [lemma.lemmatize(lem) for lem in token]\n",
    "print(lematize)                  ## it is more better than compare to (porter, lancaster or snowball  stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b89b3f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#WORDNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8b520b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "05be52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's find synonyms & Antonyms from python code\n",
    "\n",
    "synonym= []\n",
    "antonym= []\n",
    "for syn in wordnet.synsets('active'):\n",
    "    for l in syn.lemmas():\n",
    "        synonym.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonym.append(l.antonyms()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5eefa929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['active_agent', 'active', 'active_voice', 'active', 'active', 'active', 'active', 'combat-ready', 'fighting', 'active', 'active', 'participating', 'active', 'active', 'active', 'active', 'alive', 'active', 'active', 'active', 'dynamic', 'active', 'active', 'active']\n",
      "['passive_voice', 'inactive', 'passive', 'inactive', 'inactive', 'inactive', 'quiet', 'passive', 'stative', 'extinct', 'dormant', 'inactive']\n"
     ]
    }
   ],
   "source": [
    "print(synonym)\n",
    "print(antonym)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2042c6",
   "metadata": {},
   "source": [
    "# END  =  NLP-CLASS-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "36a0f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################    = ^ if these are  of any string then tokenize can't seperate it\n",
    "\n",
    "text4= \"Tokenization is= a @process^ of breaking down a given * paragraph of that into a list!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "fce264aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'a', 'process', 'of', 'breaking', 'down', 'a', 'given', 'paragraph', 'of', 'that', 'into', 'a', 'list']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(text4, \"[a-zA-Z]+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "03e9c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is', ' a @process', ' of breaking down a given * paragraph of that into a list!']\n"
     ]
    }
   ],
   "source": [
    "print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1a74c627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is']\n",
      "['a', '@', 'process']\n",
      "['of', 'breaking', 'down', 'a', 'given', '*', 'paragraph', 'of', 'that', 'into', 'a', 'list', '!']\n"
     ]
    }
   ],
   "source": [
    "for n in reg:\n",
    "#    print()\n",
    "    print(word_tokenize(n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
